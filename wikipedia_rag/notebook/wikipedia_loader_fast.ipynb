{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "from llama_index.core import Document\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import sys\n",
    "import dotenv\n",
    "dotenv.load_dotenv()\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "from llama_index.core.schema import MetadataMode\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core import VectorStoreIndex, StorageContext\n",
    "from llama_index.vector_stores.milvus import MilvusVectorStore\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "\n",
    "from pymilvus import Collection, connections\n",
    "import time\n",
    "\n",
    "import openai\n",
    "from llama_index.llms.openai import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "실행시간 :  11.857601642608643\n"
     ]
    }
   ],
   "source": [
    "# 환경변수 가져오기\n",
    "start = time.time()\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "# initialize LLM\n",
    "llm = OpenAI(model=\"gpt-4o-mini\")\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-m3\")\n",
    "\n",
    "vector_store = MilvusVectorStore(\n",
    "    uri=\"http://localhost:19530\", collection_name = \"wiki_test\", dim=1024, similarity_metric=\"COSINE\", overwrite=True)\n",
    "\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "end = time.time()\n",
    "print(\"실행시간 : \", end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "parquet_file = \"/home/livin/rag_pipeline/wikipedia_rag/data/wikipedia/wiki_ko.parquet\"\n",
    "df = pd.read_parquet(parquet_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 6416.63it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b47df0a78a044e6b8e9c093a9cdf3a33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing nodes:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5968\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56ffba7654874615b56ad85bb0a9aacf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/2048 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lang = parquet_file.split(\"wiki_\")[-1].split(\".\")[0]\n",
    "documents = []\n",
    "\n",
    "for i in tqdm(range(1000)):\n",
    "    document = Document(\n",
    "        text=df.iloc[i][\"text\"],\n",
    "        metadata={\"filename\": df.iloc[i][\"title\"], \"url\": df.iloc[i][\"url\"], \"lang\":lang},\n",
    "    )\n",
    "    document.excluded_embed_metadata_keys = [\"url\", \"lang\"]\n",
    "    document.excluded_llm_metadata_keys = [\"url\", \"lang\"]\n",
    "    documents.append(document)\n",
    "splitter = SentenceSplitter(chunk_size=1024, chunk_overlap=200)\n",
    "nodes = splitter.get_nodes_from_documents(documents, show_progress=True)\n",
    "print(len(nodes))\n",
    "\n",
    "index = VectorStoreIndex(nodes, embed_model=embed_model, storage_context=storage_context, show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "def batch_indexing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "auto_rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
