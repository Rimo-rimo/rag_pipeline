node_lines:

# Retrieval
- node_line_name: retrieve_node_line
  nodes:
    - node_type: retrieval
      strategy:
        metrics: [retrieval_f1, retrieval_recall, retrieval_precision, retrieval_ndcg, retrieval_map, retrieval_mrr]
      top_k: 15
      modules:

        - module_type: vectordb
          embedding_model: [openai_embed_3_large]


- node_line_name: post_retrieve_node_line
  nodes:
    - node_type: prompt_maker
      strategy:
        metrics:
          - metric_name: rouge
          - metric_name: sem_score
            embedding_model: huggingface_bge_m3
        generator_modules:
          - module_type: openai_llm
            llm: gpt-4o-mini
      modules:
        - module_type: fstring
          prompt:
          - | 
            단락을 읽고 질문에 답하세요. \n 질문 : {query} \n 단락: {retrieved_contents} \n 답변 :
    - node_type: generator
      strategy:
        metrics: # bert_score 및 g_eval 사용 역시 추천합니다. 빠른 실행을 위해 여기서는 제외하고 하겠습니다.
          - metric_name: rouge
          - metric_name: sem_score
            embedding_model: huggingface_bge_m3
      modules:
        - module_type: openai_llm
          llm: gpt-4o
          temperature: 0.6
          batch: 16
        - module_type: openai_llm
          llm: gpt-4o-mini
          temperature: 0.6
          batch: 16
        

# autorag evaluate --config /home/livin/rag_pipeline/AutoRAG/configs/advanced.yaml --qa_data_path /home/livin/rag_pipeline/AutoRAG/results/qa/hyundai_upstage_qa_1234.parquet --corpus_data_path /home/livin/rag_pipeline/AutoRAG/results/qa/hyundai_upstage_corpus.parquet